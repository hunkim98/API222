---
title: "API 222 Problem Set 2"
subtitle: "Machine Learning and Big Data Analytics: Fall 2024"
output: pdf_document
date: "Due at 11:59am on October 23 - submit on Gradescope"
---

```{r, include=FALSE, message=FALSE, warning=FALSE}
# The needed libraries
library(tidyverse)
library(stargazer)
library(FNN)
library(kableExtra)
library(glmnet)

# Ask R to not present numbers using scientific notation
options(scipen = 999)
```

This problem set is worth 30 points in total. To get full credit, submit your code along with a write-up of your answers. This should either be done in R Markdown or Jupyter Notebook, submitted in one knitted PDF.

## Final Project Groups (0 pts)

Please join one of 30 final project groups that have been created on this \color{blue} [Canvas](https://canvas.harvard.edu/courses/146422/groups#tab-26761) \color{black} page.

Details about the final project can be found on this \color{blue} [Canvas](https://canvas.harvard.edu/courses/146422/files/folder/Final%20Project) \color{black} page. You just need to form your group by October 23. The main project milestones will be after the midterm. We recommend forming groups of 5 students. All students working together should join the same group. PhD students need to work individually (see details on Canvas). If you are a PhD student or are otherwise working alone, please form a group by yourself. Please email Jacob if you have questions about this assignment or the final project.

## Conceptual Questions (15 pts)

1.  Consider the four main classification methods that have been presented thus far this semester: logistic regression, k-Nearest Neighbors, linear discriminant analysis (LDA), and quadratic discriminant analysis (QDA. Which of these methods may be appropriate if you know the decision boundary between the classes is linear? (3pts)

> Linear discriminant analysis (LDA) and logistic regression will be appropriate if you know the decision boundary between the classes is linear. Both LDA and logistic regression assume that the decision boundary is linear.

2.  Suppose you had the following data and you are using KNN Regression with Euclidean distance. Consider the prediction problem where you want to predict Y for the data point X1 = X2 = X3 = 0.

| X1  | X2  | X3  | Y   |
|-----|-----|-----|-----|
| 0   | 3.5 | 2   | 2   |
| 1   | 2.1 | 3   | 1   |
| 2   | 4.7 | 1   | 3   |
| 1   | 3.9 | 1   | 2   |
| 0   | 2.9 | 2   | 4   |
| 1   | 1.5 | 2   | 1   |
| 1   | 3.5 | 4   | 2   |

(a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0. (1pt)

```{r}
# code
x1 <- c(0, 1, 2, 1, 0, 1, 1)
x2 <- c(3.5, 2.1, 4.7, 3.9, 2.9, 1.5, 3.5)
x3 <- c(2, 3, 1, 1, 2, 2, 4)
xt <- c(0, 0, 0)

distances <- sqrt((x1 - xt[1])^2 + (x2 - xt[2])^2 + (x3 - xt[3])^2)

for (i in 1:length(distances)) {
  print(paste("Distance between observation", i, "and test point is", distances[i]))
}
```

(b) What is your prediction with K = 2? Why? (1pt)

> K is the number of nearest neighbors to consider. In this case, K = 2. The two nearest neighbors to the test point are the 5th (distance: 3.6) and 6th (distance: 2.7) observations. The Y values for these observations are 4 and 1, respectively. The prediction is the average of these two values, which is (4 + 1) / 2 = 2.5.

(c) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why? (1pt)

> If the Bayes decision boundary is highly nonlinear, we would expect the best value for K to be small. A small value of K will allow the model to capture the nonlinearities in the data. A large value of K would smooth out the decision boundary and may not capture the nonlinearities in the data.

3.  Consider we conduct a research study analyzing the risk factors for developing prostate cancer among men, with variables $X_1 = \text{age (years)}$, $X_2 = \text{family history of prostate cancer (0 = no, 1 = yes)}$, $X_3 = \text{smoking status (0 = non-smoker, 1 = smoker)}$, and $Y = \text{probability of developing prostate cancer}$. A logistic regression analysis is performed, resulting in estimated coefficients $\hat{\beta}_1 = 0.06$, $\hat{\beta}_2 = 1.2$, $\hat{\beta}_3 = 0.8$, and $\hat{\beta}_0 = -3.5$.

(a) Interpret $\hat{\beta}_2$. (1 pt)

> In logistic regression, the coefficients represent the change in the log-odds of the outcome which is developing prostate cancer. $\hat{\beta}_2$ is the connected to whether the person has a family history of prostate cancer. It is associated with an increase in the log-odds of developing prostate cancer by 1.2, when the person who has a family history of prostate cancer is compared to someone without such family history, while holding age and smoking status constant.

(b) Estimate the probability that a 60-year-old man with a family history of prostate cancer who is a smoker develops prostate cancer. (2 pts)

```{r}
# code
B0 <- -3.5
B1 <- 0.06
B2 <- 1.2
B3 <- 0.8

X1 <- 60
X2 <- 1
X3 <- 1

log_odds <- B0 + B1 * X1 + B2 * X2 + B3 * X3

probability <- exp(log_odds) / (1 + exp(log_odds))

probability

```


4.  k-fold cross-validation

<!-- -->

(a) Briefly explain how k-fold cross-validation is implemented. (2pts)

> In k-fold cross-validation, the data is divided into k subsets of equal size. The model is trained on k-1 of the subsets and tested on the remaining subset. This process is repeated k times, with each of the k subsets used exactly once as the validation data. The k results from the folds are then averaged to produce a single estimation of model performance.

(b) What are the advantages of k-fold cross-validation relative to the validation set approach? (1pt)

> The advantages of k-fold cross-validation relative to the validation set approach are that it provides a more accurate estimate of model performance. It uses all the data for training and testing, which can help reduce the variance of the performance estimate. It also allows for the validation of the model on multiple subsets of the data, which can help identify potential issues with overfitting.

<!-- -->

5.  Suppose you want to minimize the false negative rate in your classification. You run two models: A and B. AUC for Model A is 0.7 and for Model B is 0.8. Can you conclude that you should choose Model B? Why or why not? (3 pts)

> The AUC is a measure of the model's ability to discriminate between the positive and negative classes. A higher AUC indicates better discrimination. However, the AUC does not directly measure the false negative rate. It is possible that Model B has a higher AUC but a higher false negative rate than Model A. Therefore, we cannot conclude that we should choose Model B based solely on the AUC values. We would need to evaluate the false negative rates of both models to determine which one minimizes the false negative rate.

\newpage

## Applied Questions (15 pts)

**Predicting Hospital Length of Stay**

For the next portion of this assignment you will be working with the `LengthOfStay.csv` dataset. This dataset has data points on patients admitted into hospital, indicators of their health condition and how long they were admitted in the hospital.

This is an important problem in healthcare. In order for hospitals to optimize resource allocation, it is important to predict accurately how long a newly admitted patient will stay in the hospital.

1.  What are the dimensions of the dataset? (1 pt)

```{r, message=FALSE, warning=FALSE}
# code
df <- read.csv("LengthOfStay.csv")
dim(df)
```

2.  Use the `cor()` function to display the correlations of all **continuous** variables in the dataset. Which variables is most highly correlated with `lengthofstay`? (2 pts)

```{r}
# ignore X column from df

df <- df[, -1]

# code
continuous <- df[, sapply(df, is.numeric)] 

correlations <- cor(continuous)

print(correlations)

# most highly correlated variable with lengthofstay
cor_with_lengthofstay <- correlations["lengthofstay", ]
cor_with_lengthofstay <- cor_with_lengthofstay[order(abs(cor_with_lengthofstay), decreasing = TRUE)]

# 
print(names(cor_with_lengthofstay[2:length(cor_with_lengthofstay)]))
```
> The most highly correlated **continuous** variable with `lengthofstay` is `psychologicaldisordermajor`.


Consider the prediction problem where you want to predict the length of stay for a patient (`lengthofstay`) against all other variables available in the data set.

3.  Run ridge regression with cross-validation and standardized features using the canned function `cv.glmnet` from the package `glmnet`. You can use the $\lambda$ sequence generated by `cv.glment` (you do not need to provide your own $\lambda$ sequence). In order to receive credit for this question, make the line immediately preceding this command say `set.seed(222)` and run the two lines together. Please report all numbers by rounding to three decimal places. (2 pts)

```{r}
# code

index_of_lengthofstay <- which(colnames(df) == "lengthofstay")

set.seed(222)
model <- cv.glmnet(x = as.matrix(df[, -index_of_lengthofstay]), 
                   y = as.numeric(df[, index_of_lengthofstay]), 
                   alpha = 0, standardize = TRUE)

model
      
```

(a) Which $\lambda$ had the lowest mean cross-validation error? (1 pt)

```{r}
# code
print(paste("The lambda value is", round(model$lambda.min, 3)))
```

(b) What was the cross-validation error? (1 pt)

```{r}
# code
print(paste("The cross-validation error is", 
            round(model$cvm[model$lambda == model$lambda.min], 3)))


```

(c) What was the standard error of the mean cross-validation error for this value of $\lambda$? (1 pt)

```{r}
# code
print(paste("The standard error is", 
            round(model$cvsd[model$lambda == model$lambda.min], 3)))
```

(d) What was the largest value of $\lambda$ whose mean cross validation error was within one standard deviation of the lowest cross-validation error? (1 pt)

```{r}
# code
print(paste("The largest value of lambda is", 
            round(model$lambda.1se, 3)))
```

4.  Produce the regression coefficients for the ridge regression model with the $\lambda$ value that minimizes the cross-validation error. Compare these coefficients with a standard linear regression model. (2 pts)

```{r}
# code

# Ridge regression coefficients
ridge_coefs <- coef(model, s = model$lambda.min)
# round by 3
ridge_coefs <- round(ridge_coefs, 3)
# print(ridge_coefs)

# standard linear regression model
lm_model <- lm(lengthofstay ~ ., data = df)
lm_coefs <- coef(lm_model)
lm_coefs <- round(lm_coefs, 3)
# print(lm_coefs)

for (i in 1:length(lm_coefs)) {
  print(paste("Coefficient for", names(lm_coefs)[i], ", ridge:", lm_coefs[i],
              "standard linear regression:", ridge_coefs[i]))
}


```

5.  Now consider the same prediction problem. Implement your own 5-fold cross-validation routine for KNN for $K = 1, ..., 50$ (write the cross-validation routine yourself rather than using a canned package). Include the snippet of code you wrote here. It should not exceed 20 lines. (4pts)

```{r, message=FALSE, warning=FALSE}
# code

set.seed(222)

cross_validate_knn <- function(full_data, y_index, k_max=50, folds=5) {
  fold_ids <- rep(seq(folds), ceiling(nrow(full_data) / folds))
  fold_ids <- fold_ids[1:nrow(full_data)]
  fold_ids <- sample(fold_ids, length(fold_ids))
  cv_errors <- c()
  for (k in 1:k_max) {
    fold_errors <- c()
    for (fold in 1:folds) {
      train_data <- full_data[fold_ids != fold, ]
      test_data <- full_data[fold_ids == fold, ]
      knn_model <- knn(train = train_data[, -y_index], 
                       test = test_data[, -y_index],
                       cl = train_data[, y_index], k = k)
      fold_errors <- c(fold_errors, mean(knn_model != test_data[, y_index]))
    }
    cv_errors <- c(cv_errors, mean(fold_errors))
  }
  return (cv_errors)
}

```

(a) Plot of mean cross-validation MSE as a function of $k$.

```{r}
# code

cv_errors <- cross_validate_knn(df, index_of_lengthofstay)
# plot 
plot(1:50, cv_errors, type = "l", xlab = "k", ylab = "Mean Cross-Validation Error", main = "Mean Cross-Validation Error vs. k")
```

(b) The best k according to CV is

```{r}
# code
best_k <- which.min(cv_errors)
print(paste("The best k according to cross-validation is", best_k))
```

(c) The cross-validation error for the best k is

```{r}
# code
print(paste("The cross-validation error for the best k is", round(cv_errors[best_k], 3)))
```

\newpage

## Challenge Problem (just for fun 0 pts)

Tasks:

(a) Predict Length of Stay with LASSO Regression with Cross-Validation:

-   Use the `cv.glmnet` function from the `glmnet` package to perform LASSO regression with cross-validation.

-   Set `alpha = 1` to specify LASSO regression.

(b) Selecting the Best Model:

-   Identify the value of lambda that minimizes the cross-validation error.

-   Report this value of lambda and the corresponding cross-validation error (rounded to three decimal places).

(c) Visualization:

-   Plot the cross-validation curve (cross-validation error versus log(lambda)) using `plot(cv.glmnet_object)`.

-   Plot the coefficient paths as a function of log(lambda) to visualize how coefficients change with different lambda values.

(d) Interpretation:

-   Discuss the variables that have been selected by the LASSO model.

-   Compare these variables to those in the standard linear regression model you previously fitted.

-   Comment on any differences and provide possible explanations for why certain variables were eliminated.

```{r}
# code
```
